{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3c04e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import transform_crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade3ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_INCREMENT_FT = 2.5\n",
    "MIN_THICKNESS_FOR_INTERNAL_POINTS_FT = PREDICTION_INCREMENT_FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f280cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/x2tsq5qj4v512jqb_k8l1jgw0000gp/T/ipykernel_5306/1530089489.py:1: DtypeWarning: Columns (9,10,11,12,13,30,33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  borehole_df = pd.read_csv('../../Documents/borehole_data/subsurface_layer_data_joined.csv')\n"
     ]
    }
   ],
   "source": [
    "borehole_df = pd.read_csv('../../Documents/borehole_data/subsurface_layer_data_joined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f235cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_col = 'EPSG6599_LATITUDE'\n",
    "long_col = 'EPSG6599_LONGITUDE'\n",
    "depth_col = 'DEPTH_FT'\n",
    "lithology_col = 'SYMBOL_LITHOLOGY'\n",
    "grouped_lithology_col = 'Grouped_Lithology'\n",
    "\n",
    "feature_cols = [lat_col, long_col, depth_col]\n",
    "target_col = grouped_lithology_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5154166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 70801 rows with missing essential data.\n"
     ]
    }
   ],
   "source": [
    "initial_rows = borehole_df.shape[0]\n",
    "borehole_df.dropna(subset=['LATITUDE', 'LONGITUDE', 'TOP_DEPTH_FT', 'BOTTOM_DEPTH_FT', lithology_col], inplace=True)\n",
    "rows_after_nan_drop = borehole_df.shape[0]\n",
    "if initial_rows > rows_after_nan_drop:\n",
    "        print(f\"Dropped {initial_rows - rows_after_nan_drop} rows with missing essential data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c40c09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = '../data/boundary_2p5_interval_ungrouped_lithology_dem_training_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "979d8cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_projected_df = transform_crs(\n",
    "    borehole_df,\n",
    "    source_crs='EPSG:4326',\n",
    "    target_crs='EPSG:6599',\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0d20066",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_training_data = []\n",
    "processed_data_columns = ['EPSG6599_LATITUDE', 'EPSG6599_LONGITUDE', 'DEPTH_FT', 'SYMBOL_LITHOLOGY']\n",
    "for _, original_row in crs_projected_df.iterrows():\n",
    "    new_rows = []\n",
    "    top_depth_ft = original_row['TOP_DEPTH_FT']\n",
    "    bottom_depth_ft = original_row['BOTTOM_DEPTH_FT']\n",
    "    top_depth_row = [\n",
    "        original_row['EPSG6599_LATITUDE'],\n",
    "        original_row['EPSG6599_LONGITUDE'],\n",
    "        original_row['TOP_DEPTH_FT'], \n",
    "        original_row['SYMBOL_LITHOLOGY']\n",
    "        ]\n",
    "    bottom_depth_row = [\n",
    "        original_row['EPSG6599_LATITUDE'],\n",
    "        original_row['EPSG6599_LONGITUDE'],\n",
    "        original_row['BOTTOM_DEPTH_FT'],\n",
    "        original_row['SYMBOL_LITHOLOGY']\n",
    "    ]\n",
    "    new_rows.append(top_depth_row)\n",
    "    new_rows.append(bottom_depth_row)\n",
    "\n",
    "    layer_depth_ft = bottom_depth_ft - top_depth_ft\n",
    "    if layer_depth_ft >= MIN_THICKNESS_FOR_INTERNAL_POINTS_FT:\n",
    "        for ft in np.arange(top_depth_ft, bottom_depth_ft, PREDICTION_INCREMENT_FT):\n",
    "            layer_row = [\n",
    "                original_row['EPSG6599_LATITUDE'],\n",
    "                original_row['EPSG6599_LONGITUDE'],\n",
    "                ft,\n",
    "                original_row['SYMBOL_LITHOLOGY']\n",
    "            ]\n",
    "            new_rows.append(layer_row)\n",
    "    else:\n",
    "        midpoint_depth_ft = (top_depth_ft + bottom_depth_ft) / 2\n",
    "        midpoint_depth_row = [\n",
    "            original_row['EPSG6599_LATITUDE'],\n",
    "            original_row['EPSG6599_LONGITUDE'],\n",
    "            midpoint_depth_ft,\n",
    "            original_row['SYMBOL_LITHOLOGY']\n",
    "        ]\n",
    "        new_rows\n",
    "    processed_training_data.extend(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7979ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.DataFrame(processed_training_data, columns=processed_data_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ffe5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_for_stratify = 2\n",
    "test_size_proportion = 0.25\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fd5b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking class counts before filtering...\n",
      "Original Class Distribution (Top 20):\n",
      "SYMBOL_LITHOLOGY\n",
      "Silty sand              523796\n",
      "Sand                    219004\n",
      "Sandy silt              124880\n",
      "Silt                     92440\n",
      "Gravelly sand            77546\n",
      "Clayey silt              57867\n",
      "Silty clay               56574\n",
      "Topsoil / vegetation     45314\n",
      "Clay                     38239\n",
      "Gravel                   35948\n",
      "Undefined                26443\n",
      "Silty gravel             26412\n",
      "Peat                     22274\n",
      "Fill                     21751\n",
      "Clayey sand              19030\n",
      "Asphalt / concrete       18868\n",
      "Sedimentary bedrock      13300\n",
      "Gravelly silt             9850\n",
      "Sandy clay                9622\n",
      "Sandy gravel              8534\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nChecking class counts before filtering...\")\n",
    "class_counts = training_df[lithology_col].value_counts()\n",
    "print(\"Original Class Distribution (Top 20):\")\n",
    "print(class_counts.head(20)) # Print top common classes\n",
    "\n",
    "classes_to_remove = class_counts[class_counts < min_samples_for_stratify].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eec76e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No classes with less than 2 members found. Proceeding with split.\n"
     ]
    }
   ],
   "source": [
    "if classes_to_remove:\n",
    "    print(f\"\\nFound classes with less than {min_samples_for_stratify} members: {classes_to_remove}. These will be removed for stratification.\")\n",
    "    df_filtered = training_df[~training_df[lithology_col].isin(classes_to_remove)].copy()\n",
    "    print(f\"Data shape after removing rare classes: {df_filtered.shape}\")\n",
    "    print(\"Updated Class Distribution (Top 20):\")\n",
    "    print(df_filtered[lithology_col].value_counts().head(20))\n",
    "else:\n",
    "    print(\"\\nNo classes with less than 2 members found. Proceeding with split.\")\n",
    "    df_filtered = training_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c89d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_df = pd.read_csv('../../Documents/borehole_data/borehole_dem_features_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d61388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LONGITUDE', 'LATITUDE', 'BOREHOLE_ID', 'EPSG6599_LATITUDE',\n",
       "       'EPSG6599_LONGITUDE', 'DEM_Elevation_Feet', 'DEM_Slope_Degrees',\n",
       "       'DEM_Aspect_Degrees'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem_df = dem_df.drop(['BOREHOLE_ID', 'LONGITUDE', 'LATITUDE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89973e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_df = pd.merge(df_filtered, dem_df, how='left', on=['EPSG6599_LATITUDE', 'EPSG6599_LONGITUDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef8f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 347209 rows with missing essential data.\n"
     ]
    }
   ],
   "source": [
    "initial_rows = final_training_df.shape[0]\n",
    "final_training_df.dropna(inplace=True)\n",
    "rows_after_nan_drop = final_training_df.shape[0]\n",
    "if initial_rows > rows_after_nan_drop:\n",
    "        print(f\"Dropped {initial_rows - rows_after_nan_drop} rows with missing essential data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc254b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c38a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_group_map = {\n",
    "    '(Metamorphic bedrock)': 'Bedrock',\n",
    "    'Plutonic bedrock': 'Bedrock',\n",
    "    'Sedimentary bedrock': 'Bedrock',\n",
    "    'Volcanic bedrock': 'Bedrock',\n",
    "    'Undifferentiated rock': 'Bedrock',\n",
    "\n",
    "    'Asphalt / concrete': 'Anthropogenic',\n",
    "    'Debris': 'Anthropogenic',\n",
    "    'Fill': 'Anthropogenic',\n",
    "    'Topsoil / vegetation': 'Anthropogenic', # Or maybe 'Near-Surface' depending on context\n",
    "\n",
    "    'Peat': 'Peat', # Often kept separate due to unique properties\n",
    "\n",
    "    'Clay': 'Clayey Soils',\n",
    "    'Silty clay': 'Clayey Soils',\n",
    "    'Sandy clay': 'Clayey Soils',\n",
    "    'Gravelly clay': 'Clayey Soils',\n",
    "\n",
    "    'Silt': 'Silty Soils',\n",
    "    'Clayey silt': 'Silty Soils',\n",
    "    'Sandy silt': 'Silty Soils',\n",
    "    'Gravelly silt': 'Silty Soils',\n",
    "\n",
    "    'Sand': 'Sand (with Fines)', # Naming implies potential fines, but sand is dominant\n",
    "    'Silty sand': 'Sand (with Fines)',\n",
    "    'Clayey sand': 'Sand (with Fines)',\n",
    "\n",
    "    'Gravel': 'Gravel (with Fines)', # Naming implies potential fines, but gravel is dominant\n",
    "    'Sandy gravel': 'Gravel (with Fines)',\n",
    "    'Silty gravel': 'Gravel (with Fines)',\n",
    "    'Clayey gravel': 'Gravel (with Fines)',\n",
    "    'Cobbles / boulders': 'Gravel (with Fines)', # Or potentially a separate 'Coarse Aggregate' group\n",
    "\n",
    "    # 'Gravelly sand': 'Mixed Sand & Gravel', # Keeping this potentially separate as a common mix\n",
    "\n",
    "    'Undefined': 'Undefined/Remove', # Mark for removal\n",
    "    'Volcanic ash': 'Undefined/Remove' # Mark for removal due to rarity\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4a74ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_df[grouped_lithology_col] = final_training_df[lithology_col].map(lithology_group_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2386daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_df = final_training_df[final_training_df[grouped_lithology_col] != 'Undefined/Remove'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e69d8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking grouped class counts before filtering...\n",
      "Grouped Class Distribution:\n",
      "Grouped_Lithology\n",
      "Sand (with Fines)      586810\n",
      "Silty Soils            221088\n",
      "Clayey Soils            81608\n",
      "Anthropogenic           67969\n",
      "Gravel (with Fines)     53054\n",
      "Peat                    16942\n",
      "Bedrock                 12879\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking grouped class counts before filtering...\")\n",
    "grouped_class_counts = final_training_df[grouped_lithology_col].value_counts()\n",
    "print(\"Grouped Class Distribution:\")\n",
    "print(grouped_class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "971dce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_df.to_csv('../data/boundary_2p5_interval_grouped_lithology_dem_training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e207ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_slope_col = 'Surface_Slope' # Name of the new column from DEM features file\n",
    "surface_aspect_col = 'Surface_Aspect' # Name of the new column from DEM features file\n",
    "\n",
    "# Define features for the model (updated to include DEM features)\n",
    "# feature_cols = [lat_col, lon_col, midpoint_depth_col, midpoint_elev_col, surface_slope_col, surface_aspect_col]\n",
    "\n",
    "# New target variable name (grouped lithology)\n",
    "grouped_lithology_col = 'Grouped_Lithology'\n",
    "\n",
    "# Define the proportion of data to use for testing\n",
    "test_size_proportion = 0.25\n",
    "\n",
    "# Define a random state for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "# Minimum samples required for a grouped class to be included in training/testing with stratification\n",
    "min_samples_for_stratify = 10 # Increased minimum threshold as we have more data\n",
    "\n",
    "    # --- Separate features (X) and target (y) on the filtered data ---\n",
    "    X = df_filtered[feature_cols]\n",
    "    y = df_filtered[grouped_lithology_col]\n",
    "\n",
    "    print(f\"\\nFeatures shape (X): {X.shape}\")\n",
    "    print(f\"Target shape (y): {y.shape}\")\n",
    "    print(f\"Target variable unique values after cleaning: {y.unique().shape[0]} unique grouped lithology types.\")\n",
    "\n",
    "\n",
    "    # --- Perform the train-test split ---\n",
    "    print(f\"\\nSplitting data into training ({1 - test_size_proportion:.0%}) and testing ({test_size_proportion:.0%})...\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=test_size_proportion,\n",
    "        random_state=random_seed,\n",
    "        stratify=y # Stratify should now work on grouped lithologies\n",
    "    )\n",
    "\n",
    "    print(\"Data split complete.\")\n",
    "    print(f\"Training features shape (X_train): {X_train.shape}\")\n",
    "    print(f\"Testing features shape (X_test): {X_test.shape}\")\n",
    "    print(f\"Training target shape (y_train): {y_train.shape}\")\n",
    "    print(f\"Testing target shape (y_test): {y_test.shape}\")\n",
    "\n",
    "    # Check class distribution in train/test sets\n",
    "    print(\"\\nGrouped Class distribution comparison (Training vs. Testing):\")\n",
    "    train_dist = y_train.value_counts(normalize=True)\n",
    "    test_dist = y_test.value_counts(normalize=True)\n",
    "    dist_comparison = pd.DataFrame({'Train': train_dist, 'Test': test_dist})\n",
    "    print(dist_comparison)\n",
    "\n",
    "\n",
    "    # --- Proceed to Model Training with X_train, X_test, y_train, y_test ---\n",
    "    # The next code block will use these variables.\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Input file not found - {e}. Please check file paths.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Column '{e}' not found in one of the dataframes. Please check column names in configuration and input files.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Data or Configuration Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046811ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_filtered.shape[0] == 0:\n",
    "        raise ValueError(\"No data remaining after cleaning and removing rare classes. Cannot perform split.\")\n",
    "\n",
    "# --- Separate features (X) and target (y) on the filtered data ---\n",
    "X = df_filtered[feature_cols]\n",
    "y = df_filtered[lithology_col]\n",
    "\n",
    "print(f\"\\nFeatures shape (X): {X.shape}\")\n",
    "print(f\"Target shape (y): {y.shape}\")\n",
    "print(f\"Target variable unique values after cleaning: {y.unique().shape[0]} unique lithology types.\")\n",
    "\n",
    "\n",
    "# --- Perform the train-test split ---\n",
    "print(f\"\\nSplitting data into training ({1 - test_size_proportion:.0%}) and testing ({test_size_proportion:.0%})...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=test_size_proportion,\n",
    "    random_state=random_seed,\n",
    "    stratify=y # Stratify should now work\n",
    ")\n",
    "\n",
    "print(\"Data split complete.\")\n",
    "print(f\"Training features shape (X_train): {X_train.shape}\")\n",
    "print(f\"Testing features shape (X_test): {X_test.shape}\")\n",
    "print(f\"Training target shape (y_train): {y_train.shape}\")\n",
    "print(f\"Testing target shape (y_test): {y_test.shape}\")\n",
    "\n",
    "# Check class distribution in train/test sets\n",
    "print(\"\\nClass distribution comparison (Training vs. Testing):\")\n",
    "train_dist = y_train.value_counts(normalize=True)\n",
    "test_dist = y_test.value_counts(normalize=True)\n",
    "dist_comparison = pd.DataFrame({'Train': train_dist, 'Test': test_dist})\n",
    "print(dist_comparison.head()) # Print comparison for top classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
