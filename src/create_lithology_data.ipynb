{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e3c04e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f280cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ct/x2tsq5qj4v512jqb_k8l1jgw0000gp/T/ipykernel_64517/1530089489.py:1: DtypeWarning: Columns (9,10,11,12,13,30,33,34) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  borehole_df = pd.read_csv('../../Documents/borehole_data/subsurface_layer_data_joined.csv')\n"
     ]
    }
   ],
   "source": [
    "# borehole_df = pd.read_csv('../../Documents/borehole_data/subsurface_layer_data_joined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_csv_path = '../data/full_lithology_training_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ffe5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_for_stratify = 2\n",
    "test_size_proportion = 0.25\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df372664",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_col = 'LATITUDE'\n",
    "lon_col = 'LONGITUDE'\n",
    "collar_elev_col = 'ELEVATION_FT'\n",
    "top_depth_col = 'TOP_DEPTH_FT'\n",
    "bottom_depth_col = 'BOTTOM_DEPTH_FT'\n",
    "lithology_col = 'SYMBOL_LITHOLOGY'\n",
    "\n",
    "midpoint_depth_col = 'Midpoint_Depth_ft'\n",
    "midpoint_elev_col = 'Midpoint_Elevation_ft'\n",
    "\n",
    "feature_cols = [lat_col, lon_col, midpoint_depth_col, midpoint_elev_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a04dba67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410677, 39)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "borehole_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e851e71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 70801 rows with missing essential data.\n"
     ]
    }
   ],
   "source": [
    "initial_rows = borehole_df.shape[0]\n",
    "borehole_df.dropna(subset=[collar_elev_col, top_depth_col, bottom_depth_col, lithology_col, lat_col, lon_col], inplace=True)\n",
    "rows_after_nan_drop = borehole_df.shape[0]\n",
    "if initial_rows > rows_after_nan_drop:\n",
    "        print(f\"Dropped {initial_rows - rows_after_nan_drop} rows with missing essential data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d258ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating midpoint depth and elevation...\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating midpoint depth and elevation...\")\n",
    "borehole_df[midpoint_depth_col] = (borehole_df[top_depth_col] + borehole_df[bottom_depth_col]) / 2\n",
    "borehole_df[midpoint_elev_col] = borehole_df[collar_elev_col] - borehole_df[midpoint_depth_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd5b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking class counts before filtering...\n",
      "Original Class Distribution (Top 20):\n",
      "SYMBOL_LITHOLOGY\n",
      "Silty sand              123744\n",
      "Sand                     43099\n",
      "Sandy silt               29899\n",
      "Silt                     22086\n",
      "Topsoil / vegetation     22029\n",
      "Gravelly sand            17388\n",
      "Clayey silt              11088\n",
      "Gravel                    9785\n",
      "Asphalt / concrete        9301\n",
      "Silty clay                8808\n",
      "Clay                      7233\n",
      "Silty gravel              6433\n",
      "Fill                      5589\n",
      "Peat                      5280\n",
      "Clayey sand               3891\n",
      "Undefined                 3354\n",
      "Sedimentary bedrock       2606\n",
      "Gravelly silt             2299\n",
      "Sandy clay                2098\n",
      "Sandy gravel              1613\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nChecking class counts before filtering...\")\n",
    "class_counts = borehole_df[lithology_col].value_counts()\n",
    "print(\"Original Class Distribution (Top 20):\")\n",
    "print(class_counts.head(20)) # Print top common classes\n",
    "\n",
    "classes_to_remove = class_counts[class_counts < min_samples_for_stratify].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eec76e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No classes with less than 2 members found. Proceeding with split.\n"
     ]
    }
   ],
   "source": [
    "if classes_to_remove:\n",
    "    print(f\"\\nFound classes with less than {min_samples_for_stratify} members: {classes_to_remove}. These will be removed for stratification.\")\n",
    "    df_filtered = borehole_df[~borehole_df[lithology_col].isin(classes_to_remove)].copy()\n",
    "    print(f\"Data shape after removing rare classes: {df_filtered.shape}\")\n",
    "    print(\"Updated Class Distribution (Top 20):\")\n",
    "    print(df_filtered[lithology_col].value_counts().head(20))\n",
    "else:\n",
    "    print(\"\\nNo classes with less than 2 members found. Proceeding with split.\")\n",
    "    df_filtered = borehole_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc254b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1d7b0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features shape (X): (339876, 4)\n",
      "Target shape (y): (339876,)\n",
      "Target variable unique values after cleaning: 29 unique lithology types.\n",
      "\n",
      "Splitting data into training (75%) and testing (25%)...\n",
      "Data split complete.\n",
      "Training features shape (X_train): (254907, 4)\n",
      "Testing features shape (X_test): (84969, 4)\n",
      "Training target shape (y_train): (254907,)\n",
      "Testing target shape (y_test): (84969,)\n",
      "\n",
      "Class distribution comparison (Training vs. Testing):\n",
      "                         Train      Test\n",
      "SYMBOL_LITHOLOGY                        \n",
      "Silty sand            0.364086  0.364086\n",
      "Sand                  0.126807  0.126811\n",
      "Sandy silt            0.087969  0.087973\n",
      "Silt                  0.064981  0.064988\n",
      "Topsoil / vegetation  0.064816  0.064812\n"
     ]
    }
   ],
   "source": [
    "if df_filtered.shape[0] == 0:\n",
    "        raise ValueError(\"No data remaining after cleaning and removing rare classes. Cannot perform split.\")\n",
    "\n",
    "# --- Separate features (X) and target (y) on the filtered data ---\n",
    "X = df_filtered[feature_cols]\n",
    "y = df_filtered[lithology_col]\n",
    "\n",
    "print(f\"\\nFeatures shape (X): {X.shape}\")\n",
    "print(f\"Target shape (y): {y.shape}\")\n",
    "print(f\"Target variable unique values after cleaning: {y.unique().shape[0]} unique lithology types.\")\n",
    "\n",
    "\n",
    "# --- Perform the train-test split ---\n",
    "print(f\"\\nSplitting data into training ({1 - test_size_proportion:.0%}) and testing ({test_size_proportion:.0%})...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=test_size_proportion,\n",
    "    random_state=random_seed,\n",
    "    stratify=y # Stratify should now work\n",
    ")\n",
    "\n",
    "print(\"Data split complete.\")\n",
    "print(f\"Training features shape (X_train): {X_train.shape}\")\n",
    "print(f\"Testing features shape (X_test): {X_test.shape}\")\n",
    "print(f\"Training target shape (y_train): {y_train.shape}\")\n",
    "print(f\"Testing target shape (y_test): {y_test.shape}\")\n",
    "\n",
    "# Check class distribution in train/test sets\n",
    "print(\"\\nClass distribution comparison (Training vs. Testing):\")\n",
    "train_dist = y_train.value_counts(normalize=True)\n",
    "test_dist = y_test.value_counts(normalize=True)\n",
    "dist_comparison = pd.DataFrame({'Train': train_dist, 'Test': test_dist})\n",
    "print(dist_comparison.head()) # Print comparison for top classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c38a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "lithology_group_map = {\n",
    "    '(Metamorphic bedrock)': 'Bedrock',\n",
    "    'Plutonic bedrock': 'Bedrock',\n",
    "    'Sedimentary bedrock': 'Bedrock',\n",
    "    'Volcanic bedrock': 'Bedrock',\n",
    "    'Undifferentiated rock': 'Bedrock',\n",
    "\n",
    "    'Debris': 'Anthropogenic Fill/Debris',\n",
    "    'Fill': 'Anthropogenic Fill/Debris',\n",
    "    'Asphalt / concrete': 'Asphalt / concrete',\n",
    "    'Topsoil / vegetation': 'Topsoil / vegetation',\n",
    "\n",
    "    'Peat': 'Peat',\n",
    "\n",
    "    'Clay': 'Clayey Soils',\n",
    "    'Silty clay': 'Clayey Soils',\n",
    "    'Sandy clay': 'Clayey Soils',\n",
    "    'Gravelly clay': 'Clayey Soils',\n",
    "\n",
    "    'Silt': 'Silty Soils',\n",
    "    'Clayey silt': 'Silty Soils',\n",
    "    'Sandy silt': 'Silty Soils',\n",
    "    'Gravelly silt': 'Silty Soils',\n",
    "\n",
    "    'Sand': 'Sand (with Fines)', # Naming implies potential fines, but sand is dominant\n",
    "    'Silty sand': 'Sand (with Fines)',\n",
    "    'Clayey sand': 'Sand (with Fines)',\n",
    "\n",
    "    'Gravel': 'Gravel (with Fines)', # Naming implies potential fines, but gravel is dominant\n",
    "    'Sandy gravel': 'Gravel (with Fines)',\n",
    "    'Silty gravel': 'Gravel (with Fines)',\n",
    "    'Clayey gravel': 'Gravel (with Fines)',\n",
    "    'Cobbles / boulders': 'Gravel (with Fines)', # Or potentially a separate 'Coarse Aggregate' group\n",
    "\n",
    "    # 'Gravelly sand': 'Mixed Sand & Gravel', # Keeping this potentially separate as a common mix\n",
    "\n",
    "    'Undefined': 'Undefined/Remove', # Mark for removal\n",
    "    'Volcanic ash': 'Undefined/Remove' # Mark for removal due to rarity\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0bf3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_lithology_col = 'Grouped_Lithology'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4a74ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[grouped_lithology_col] = df_filtered[lithology_col].map(lithology_group_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2386daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[df_filtered[grouped_lithology_col] != 'Undefined/Remove'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0da7d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_rows = df_filtered.shape[0]\n",
    "df_filtered.dropna(subset=[collar_elev_col, top_depth_col, bottom_depth_col, grouped_lithology_col, lat_col, lon_col], inplace=True)\n",
    "rows_after_nan_drop = df_filtered.shape[0]\n",
    "if initial_rows > rows_after_nan_drop:\n",
    "        print(f\"Dropped {initial_rows - rows_after_nan_drop} rows with missing essential data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e69d8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking grouped class counts before filtering...\n",
      "Grouped Class Distribution:\n",
      "Grouped_Lithology\n",
      "Sand (with Fines)            170734\n",
      "Silty Soils                   65372\n",
      "Topsoil / vegetation          22029\n",
      "Clayey Soils                  18585\n",
      "Gravel (with Fines)           18356\n",
      "Asphalt / concrete             9301\n",
      "Anthropogenic Fill/Debris      6235\n",
      "Peat                           5280\n",
      "Bedrock                        3222\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking grouped class counts before filtering...\")\n",
    "grouped_class_counts = df_filtered[grouped_lithology_col].value_counts()\n",
    "print(\"Grouped Class Distribution:\")\n",
    "print(grouped_class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "971dce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv('../data/full_grouped_lithology_training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e207ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_slope_col = 'Surface_Slope' # Name of the new column from DEM features file\n",
    "surface_aspect_col = 'Surface_Aspect' # Name of the new column from DEM features file\n",
    "\n",
    "# Define features for the model (updated to include DEM features)\n",
    "# feature_cols = [lat_col, lon_col, midpoint_depth_col, midpoint_elev_col, surface_slope_col, surface_aspect_col]\n",
    "\n",
    "# New target variable name (grouped lithology)\n",
    "grouped_lithology_col = 'Grouped_Lithology'\n",
    "\n",
    "# Define the proportion of data to use for testing\n",
    "test_size_proportion = 0.25\n",
    "\n",
    "# Define a random state for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "# Minimum samples required for a grouped class to be included in training/testing with stratification\n",
    "min_samples_for_stratify = 10 # Increased minimum threshold as we have more data\n",
    "\n",
    "# --- Lithology Grouping Dictionary ---\n",
    "# Define how original lithologies map to new grouped categories\n",
    "# **Adjust this dictionary based on your refined grouping strategy**\n",
    "lithology_group_map = {\n",
    "    '(Metamorphic bedrock)': 'Bedrock',\n",
    "    'Plutonic bedrock': 'Bedrock',\n",
    "    'Sedimentary bedrock': 'Bedrock',\n",
    "    'Volcanic bedrock': 'Bedrock',\n",
    "    'Undifferentiated rock': 'Bedrock',\n",
    "\n",
    "    'Asphalt / concrete': 'Anthropogenic',\n",
    "    'Debris': 'Anthropogenic',\n",
    "    'Fill': 'Anthropogenic',\n",
    "    'Topsoil / vegetation': 'Anthropogenic', # Or maybe 'Near-Surface' depending on context\n",
    "\n",
    "    'Peat': 'Peat', # Often kept separate due to unique properties\n",
    "\n",
    "    'Clay': 'Clayey Soils',\n",
    "    'Silty clay': 'Clayey Soils',\n",
    "    'Sandy clay': 'Clayey Soils',\n",
    "    'Gravelly clay': 'Clayey Soils',\n",
    "\n",
    "    'Silt': 'Silty Soils',\n",
    "    'Clayey silt': 'Silty Soils',\n",
    "    'Sandy silt': 'Silty Soils',\n",
    "    'Gravelly silt': 'Silty Soils',\n",
    "\n",
    "    'Sand': 'Sand (with Fines)', # Naming implies potential fines, but sand is dominant\n",
    "    'Silty sand': 'Sand (with Fines)',\n",
    "    'Clayey sand': 'Sand (with Fines)',\n",
    "\n",
    "    'Gravel': 'Gravel (with Fines)', # Naming implies potential fines, but gravel is dominant\n",
    "    'Sandy gravel': 'Gravel (with Fines)',\n",
    "    'Silty gravel': 'Gravel (with Fines)',\n",
    "    'Clayey gravel': 'Gravel (with Fines)',\n",
    "    'Cobbles / boulders': 'Gravel (with Fines)', # Or potentially a separate 'Coarse Aggregate' group\n",
    "\n",
    "    # 'Gravelly sand': 'Mixed Sand & Gravel', # Keeping this potentially separate as a common mix\n",
    "\n",
    "    'Undefined': 'Undefined/Remove', # Mark for removal\n",
    "    'Volcanic ash': 'Undefined/Remove' # Mark for removal due to rarity\n",
    "}\n",
    "\n",
    "\n",
    "    # --- Data Cleaning and Feature Engineering ---\n",
    "    # Merge main data with DEM features\n",
    "    print(f\"Merging main data with DEM features on '{borehole_id_col}'...\")\n",
    "    # Ensure boreholes can be matched - assumes borehole_id_col is consistent\n",
    "    # If matching on Lat/Lon, might need rounding or different merge strategy\n",
    "    df_merged = pd.merge(df_large, df_dem[[borehole_id_col, surface_slope_col, surface_aspect_col]], on=borehole_id_col, how='left')\n",
    "    print(f\"Shape after merge: {df_merged.shape}\")\n",
    "    # Handle cases where no DEM data was found for a borehole (will result in NaNs in new columns)\n",
    "    if df_merged[[surface_slope_col, surface_aspect_col]].isnull().sum().sum() > 0:\n",
    "         print(\"\\nWarning: Missing DEM feature values found after merge. Consider imputing or dropping rows.\")\n",
    "         print(df_merged[[surface_slope_col, surface_aspect_col]].isnull().sum())\n",
    "         # For now, let's drop rows missing DEM features to keep it simple\n",
    "         merge_initial_rows = df_merged.shape[0]\n",
    "         df_merged.dropna(subset=[surface_slope_col, surface_aspect_col], inplace=True)\n",
    "         merge_rows_after_drop = df_merged.shape[0]\n",
    "         if merge_initial_rows > merge_rows_after_drop:\n",
    "              print(f\"Dropped {merge_initial_rows - merge_rows_after_drop} rows missing DEM features.\")\n",
    "\n",
    "\n",
    "    # Drop rows where essential data for layer definition or original lithology is missing\n",
    "    initial_rows = df_merged.shape[0]\n",
    "    df_merged.dropna(subset=[collar_elev_col, top_depth_col, bottom_depth_col, original_lithology_col, lat_col, lon_col], inplace=True)\n",
    "    rows_after_nan_drop = df_merged.shape[0]\n",
    "    if initial_rows > rows_after_nan_drop:\n",
    "         print(f\"Dropped {initial_rows - rows_after_nan_drop} rows with missing essential data.\")\n",
    "\n",
    "\n",
    "    # Calculate Midpoint Depth and Midpoint Elevation\n",
    "    print(\"Calculating midpoint depth and elevation...\")\n",
    "    df_merged[midpoint_depth_col] = (df_merged[top_depth_col] + df_merged[bottom_depth_col]) / 2\n",
    "    df_merged[midpoint_elev_col] = df_merged[collar_elev_col] - df_merged[midpoint_depth_col]\n",
    "\n",
    "    # --- Apply Lithology Grouping ---\n",
    "    print(\"\\nApplying lithology grouping...\")\n",
    "    df_merged[grouped_lithology_col] = df_merged[original_lithology_col].map(lithology_group_map).fillna('Other/Unknown') # Map & handle potential unmapped values\n",
    "\n",
    "    # --- Handle Rare/Undefined Grouped Classes for Stratification ---\n",
    "    print(\"Checking grouped class counts before filtering...\")\n",
    "    grouped_class_counts = df_merged[grouped_lithology_col].value_counts()\n",
    "    print(\"Grouped Class Distribution:\")\n",
    "    print(grouped_class_counts)\n",
    "\n",
    "    # Remove the 'Undefined/Remove' group explicitly\n",
    "    df_filtered = df_merged[df_merged[grouped_lithology_col] != 'Undefined/Remove'].copy()\n",
    "\n",
    "    # Also remove any remaining grouped classes with less than min_samples_for_stratify members\n",
    "    grouped_class_counts_after_removal = df_filtered[grouped_lithology_col].value_counts()\n",
    "    rare_grouped_classes = grouped_class_counts_after_removal[grouped_class_counts_after_removal < min_samples_for_stratify].index.tolist()\n",
    "\n",
    "    if rare_grouped_classes:\n",
    "        print(f\"\\nFound grouped classes with less than {min_samples_for_stratify} members after initial removal: {rare_grouped_classes}. These will be removed for stratification.\")\n",
    "        df_filtered = df_filtered[~df_filtered[grouped_lithology_col].isin(rare_grouped_classes)].copy()\n",
    "        print(f\"Data shape after removing rare grouped classes: {df_filtered.shape}\")\n",
    "        print(\"Final Grouped Class Distribution:\")\n",
    "        print(df_filtered[grouped_lithology_col].value_counts())\n",
    "    else:\n",
    "        print(\"\\nNo grouped classes with less than minimum samples found after initial removal. Proceeding with split.\")\n",
    "\n",
    "\n",
    "    # Ensure there's still data left to split\n",
    "    if df_filtered.shape[0] == 0:\n",
    "         raise ValueError(\"No data remaining after cleaning, merging, and removing rare grouped classes. Cannot perform split.\")\n",
    "\n",
    "    # --- Separate features (X) and target (y) on the filtered data ---\n",
    "    X = df_filtered[feature_cols]\n",
    "    y = df_filtered[grouped_lithology_col]\n",
    "\n",
    "    print(f\"\\nFeatures shape (X): {X.shape}\")\n",
    "    print(f\"Target shape (y): {y.shape}\")\n",
    "    print(f\"Target variable unique values after cleaning: {y.unique().shape[0]} unique grouped lithology types.\")\n",
    "\n",
    "\n",
    "    # --- Perform the train-test split ---\n",
    "    print(f\"\\nSplitting data into training ({1 - test_size_proportion:.0%}) and testing ({test_size_proportion:.0%})...\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=test_size_proportion,\n",
    "        random_state=random_seed,\n",
    "        stratify=y # Stratify should now work on grouped lithologies\n",
    "    )\n",
    "\n",
    "    print(\"Data split complete.\")\n",
    "    print(f\"Training features shape (X_train): {X_train.shape}\")\n",
    "    print(f\"Testing features shape (X_test): {X_test.shape}\")\n",
    "    print(f\"Training target shape (y_train): {y_train.shape}\")\n",
    "    print(f\"Testing target shape (y_test): {y_test.shape}\")\n",
    "\n",
    "    # Check class distribution in train/test sets\n",
    "    print(\"\\nGrouped Class distribution comparison (Training vs. Testing):\")\n",
    "    train_dist = y_train.value_counts(normalize=True)\n",
    "    test_dist = y_test.value_counts(normalize=True)\n",
    "    dist_comparison = pd.DataFrame({'Train': train_dist, 'Test': test_dist})\n",
    "    print(dist_comparison)\n",
    "\n",
    "\n",
    "    # --- Proceed to Model Training with X_train, X_test, y_train, y_test ---\n",
    "    # The next code block will use these variables.\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Input file not found - {e}. Please check file paths.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Column '{e}' not found in one of the dataframes. Please check column names in configuration and input files.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Data or Configuration Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
